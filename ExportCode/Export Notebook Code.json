{
  "paragraphs": [
    {
      "title": "Export Notebook Code Utility",
      "text": "%md\nThe **\"Export Notebook Code\"** notebook exports code from a notebook. The notebook takes in different arguments. Arguments are documented below. Utility skips exporting context for following interpreters [ %angular, %dep, %md, %sh, %knitr, %spark.knitr, %spark.sql and %sql ]. For spark 2.0 and above.\n\n**Useful for following usecases –**\n- Export ETL development done via Notebooks to run via Spark Submit command.\n- Share reusable code via a notebook (Python \u0026 R). Export the code to \".py\" or \".r\" file and then use sc.addPyFile(\"cloud_storage_path_of_exported_code_file\") or sc.addFile(\"cloud_storage_path_of_exported_code_file\") respectively to import code in other notebooks.\n- To perform code comparisons.\n\n**Limitations -**\n- It does not generate spark submit command for scala notebooks as it requires compiling jar.\n- If code uses zeppelin context (z.), this utility does not take any action on it.\n- It does not take into account dependenices set via %dep interpreter.\n\n",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1538105776784_-788350061",
      "id": "20180928-033616_1651198540_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe \u003cstrong\u003e\u0026ldquo;Export Notebook Code\u0026rdquo;\u003c/strong\u003e notebook exports code from a notebook. The notebook takes in different arguments. Arguments are documented below. Utility skips exporting context for following interpreters [ %angular, %dep, %md, %sh, %knitr, %spark.knitr, %spark.sql and %sql ]. For spark 2.0 and above.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUseful for following usecases –\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExport ETL development done via Notebooks to run via Spark Submit command.\u003c/li\u003e\n\u003cli\u003eShare reusable code via a notebook (Python \u0026amp; R). Export the code to \u0026ldquo;.py\u0026rdquo; or \u0026ldquo;.r\u0026rdquo; file and then use sc.addPyFile(\u0026ldquo;cloud_storage_path_of_exported_code_file\u0026rdquo;) or sc.addFile(\u0026ldquo;cloud_storage_path_of_exported_code_file\u0026rdquo;) respectively to import code in other notebooks.\u003c/li\u003e\n\u003cli\u003eTo perform code comparisons.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eLimitations -\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIt does not generate spark submit command for scala notebooks as it requires compiling jar.\u003c/li\u003e\n\u003cli\u003eIf code uses zeppelin context (z.), this utility does not take any action on it.\u003c/li\u003e\n\u003cli\u003eIt does not take into account dependenices set via %dep interpreter.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Sep 28, 2018 3:36:16 AM",
      "dateSubmitted": "May 20, 2019 3:53:14 PM",
      "dateStarted": "May 20, 2019 3:53:14 PM",
      "dateFinished": "May 20, 2019 3:53:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "1. Set Input Parameters",
      "text": "%pyspark\n# Notebook takes in 6 arguments –\n# 1. notebookName – Name of the notebook whose code\u0027s need to be exported. It\u0027s mandatory.\n# 2. outputLang - The language (python,scala,r) of the notebook.  You can also specify \"ALL\" to export code for comparison.\n# 3. cloudStorageFile – Filename along with its cloud path (s3,blob,adls,etc) to write the exported code. It\u0027s not mandatory. When value set to None or empty, exported code will be displayed on the console.\n# 4. addSparkSession – Adds spark session code based on language to exported code. Default value is \"Y\". \n# 5. generateSparkSubmit – Generates spark submit code to run via Qubole Analyze Shell command. Default value is \"Y\". It\u0027s disabled when outputLang is \"All\" or \"Scala\".\n# 6. moveSparkConfToSparkSession – Move sparkConf code to spark submit. Default value is \"N\" which means spark configs are added to spark session within the exported code.\n\nnotebookName\u003d\u0027SparkGraphFrames - Python\u0027\noutputLang\u003d\u0027python\u0027\ncloudStorageFile\u003d\u0027s3://qubole-sbang/mydata/sparkgraphframes.py\u0027\naddSparkSession\u003d\u0027Y\u0027\ngenerateSparkSubmit\u003d\u0027Y\u0027\nmoveSparkConfToSparkSession\u003d\u0027N\u0027",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1538941795156_-113781437",
      "id": "20181007-194955_103595655_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Oct 7, 2018 7:49:55 PM",
      "dateSubmitted": "May 20, 2019 3:53:14 PM",
      "dateStarted": "May 20, 2019 3:53:14 PM",
      "dateFinished": "May 20, 2019 3:53:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "2. Define all functions",
      "text": "%pyspark\n\nimport json,os,sys,subprocess,datetime\n    \ndef getNoteboookPath(notebookName):\n    cnt \u003d 0\n    notebookPath \u003d os.environ[\u0027ZEPPELIN_HOME\u0027] + \u0027/notebook\u0027\n    #print(notebookPath)\n    for root, dirs, files in os.walk(notebookPath):    \n        for f in files:                        \n            if f.lower().endswith((\".json\")):                    \n                filename \u003d os.path.join(root, f)\n                #print(filename)\n                f \u003d open(filename)\n                data \u003d json.load(f)         \n                f.close()\n                #print(data[\u0027name\u0027])\n                if (notebookName \u003d\u003d data[\u0027name\u0027]):\n                    cnt \u003d cnt + 1\n                    notebookPath \u003d filename\n    if cnt \u003e 1:\n        print(\"More than one notebook exists with same name %s\" %notebookName )\n        exit(-1)\n    elif  cnt \u003d\u003d 0:\n        print(\"Notebook not found\")\n        exit(-1)\n    else:\n        print(\"Found notebook\")\n        return notebookPath\n        \ndef printFile(fileprint):\n    file \u003d open(fileprint, \"r\") \n    print(file.read()) \n    file.close()\n    \ndef writeToCloudStorageFile(fileprint,cloudStorageFile):\n    p \u003d subprocess.Popen(\u0027hadoop dfs -put -f %s %s\u0027%(fileprint,cloudStorageFile), shell\u003dTrue, stdout\u003dsubprocess.PIPE, stderr\u003dsubprocess.STDOUT)\n    stdOut \u003d p.stdout.readlines()\n    retval \u003d p.wait()\n    if retval \u003d\u003d 0:\n       print(\"Successfully exported code to cloud storage file!\")\n    else:\n       print(\"Failed to write file to cloud storage - \")    \n       for line in stdOut:\n           print(line)\n           \ndef parseInterpreter(notebook_id):\n    interpreterPath \u003d os.environ[\u0027ZEPPELIN_HOME\u0027] + \u0027/conf/interpreter.json\u0027\n    #print(interpreterPath)\n    repToSkipList \u003d [\u0027file:///root/.m2/repository\u0027,\u0027http://repo1.maven.org/maven2/\u0027,\u0027file:///home/qubole-mason/.m2/repository\u0027]\n    fIntp \u003d open(interpreterPath)\n    dataIntp \u003d json.load(fIntp)\n    fIntp.close()\n    packages \u003d []\n    exclusions \u003d []\n    repositories \u003d []\n    sparkProperties \u003d {}\n    sparkPropertiesAll \u003d {}\n    #print(notebook_id)\n    if notebook_id in dataIntp[\u0027interpreterBindings\u0027]:\n        print(\u0027Attached Interpreters found\u0027)\n        intpList \u003d dataIntp[\u0027interpreterBindings\u0027][notebook_id]\n        #print(intpList)\n        for intp in intpList:\n            if intp in dataIntp[\u0027interpreterSettings\u0027]:\n                if dataIntp[\u0027interpreterSettings\u0027][intp][\u0027group\u0027] \u003d\u003d \"spark\":\n                    print(\"Found Spark Interpreter\")\n                    #print(intp)\n                    sparkPropertiesAll \u003d dataIntp[\u0027interpreterSettings\u0027][intp][\u0027properties\u0027]\n                    sparkProperties \u003d sparkPropertiesAll.copy()\n                    #print(sparkProperties)\n                    for key in sparkPropertiesAll.keys():\n                        if key in [\u0027spark.qubole.idle.timeout\u0027,\u0027args\u0027] or key.startswith(\u0027zeppelin.\u0027) or key.startswith(\u0027cluster_env\u0027):\n                            sparkProperties.pop(key)\n                    #print(sparkProperties)\n                    if \u0027dependencies\u0027 in dataIntp[\u0027interpreterSettings\u0027][intp]:\n                        for i in range (0, len (dataIntp[\u0027interpreterSettings\u0027][intp][\u0027dependencies\u0027])):\n                            packages.append(dataIntp[\u0027interpreterSettings\u0027][intp][\u0027dependencies\u0027][i][\u0027groupArtifactVersion\u0027])\n                            if \u0027exclusions\u0027 in dataIntp[\u0027interpreterSettings\u0027][intp][\u0027dependencies\u0027][i]:\n                                exclList \u003d dataIntp[\u0027interpreterSettings\u0027][intp][\u0027dependencies\u0027][i][\u0027exclusions\u0027]\n                                #print(exclList)\n                                trimExclList \u003d []\n                                for excl in exclList:\n                                    exL \u003d excl.split(\u0027:\u0027)\n                                    #print(exL)\n                                    if len(exL)\u003e2 :\n                                        trimExclList.append(exL[0]+\u0027:\u0027+exL[1])\n                                    else:\n                                        trimExclList.append(excl)\n                                exclusions.extend(trimExclList)\n                    if \u0027interpreterRepositories\u0027 in dataIntp:\n                        for i in range (0, len (dataIntp[\u0027interpreterRepositories\u0027])):\n                            if dataIntp[\u0027interpreterRepositories\u0027][i][\u0027url\u0027] not in repToSkipList:\n                                repositories.extend([dataIntp[\u0027interpreterRepositories\u0027][i][\u0027url\u0027]])\n                    return packages, exclusions, repositories, sparkProperties, sparkPropertiesAll         \n    else:\n        print(\"No attached interpreter found\")\n        exit(-1)\n       \n\ndef validateInputs(notebookName,outputLang,addSparkSession,generateSparkSubmit,moveSparkConfToSparkSession):\n    \n    validValues \u003d [\u0027Y\u0027,\u0027N\u0027]\n    all_langs \u003d [\u0027PYTHON\u0027,\u0027SCALA\u0027,\u0027R\u0027,\u0027ALL\u0027]\n    \n    if notebookName \u003d\u003d \u0027\u0027 or notebookName \u003d\u003d None:\n        print(\"No notebook name provided\")\n        exit(-1)\n        \n    if outputLang not in all_langs:\n        print(\"Valid values for outputLang are \" + all_langs)\n        exit(-1)\n    \n    if addSparkSession not in validValues:\n        print(\"Valid values for addSparkSession are Y or N\")\n        exit(-1)\n    \n    if generateSparkSubmit not in validValues:\n        print(\"Valid values for generateSparkSubmit are Y or N\")\n        exit(-1)\n    \n    if moveSparkConfToSparkSession not in validValues:\n        print(\"Valid values for moveSparkConfToSparkSession are Y or N\")\n        exit(-1)\n\n\ndef generateNotebookCode ( notebookName, cloudStorageFile, outputLang,addSparkSession\u003d\"Y\",generateSparkSubmit\u003d\"Y\",moveSparkConfToSparkSession\u003d\"N\"):\n\n    outputLang \u003d outputLang.upper()\n    addSparkSession \u003d addSparkSession.upper()\n    generateSparkSubmit \u003d generateSparkSubmit.upper()\n    moveSparkConfToSparkSession \u003d moveSparkConfToSparkSession.upper()\n    \n    if len(cloudStorageFile) \u003d\u003d 0:\n        cloudStorageFile \u003d None\n    \n    validateInputs(notebookName,outputLang,addSparkSession,generateSparkSubmit,moveSparkConfToSparkSession)\n    \n    cmdLine \u003d \u0027\u0027    \n    if outputLang \u003d\u003d \u0027ALL\u0027:\n        cmdLine \u003d\u0027Spark Sumbit command generation is not supported for languages - All\u0027\n        generateSparkSubmit \u003d \u0027N\u0027\n        moveSparkConfToSparkSession\u003d\u0027Y\u0027\n    if outputLang \u003d\u003d \u0027SCALA\u0027:\n        comment \u003d \u0027//\u0027\n        if generateSparkSubmit \u003d\u003d \u0027Y\u0027:\n            moveSparkConfToSparkSession\u003d\u0027N\u0027\n    else:\n        comment \u003d \u0027#\u0027\n        \n\n\n    #get notebook path\n    notebookFile\u003dgetNoteboookPath(notebookName)\n    \n    #generate temp filename\n    now \u003d datetime.datetime.now()\n    tmpfilename \u003d \u0027/tmp/notebook_\u0027 + now.strftime(\"%Y%m%d%H%M%s\")\n    print(\"Temp filename - %s\" %tmpfilename)\n    \n    #get json\n    f \u003d open(notebookFile)\n    data \u003d json.load(f)\n    f.close()\n\n    fout \u003d open( tmpfilename, \"w\")\n    \n    #get notebook name\n    #notebookName \u003d data[\u0027name\u0027]\n    #get notebook id\n    notebook_id \u003d data[\u0027id\u0027]  \n    #print(notebook_id)\n    fout.write(\u0027\\n%s *** Notebook Name - %s ***\u0027%(comment,notebookName))\n    fout.write(\u0027\\n%s *** Notebook Id - %s ***\u0027%(comment,notebook_id))\n    \n    packages, exclusions, repositories, sparkProperties, sparkPropertiesAll \u003d parseInterpreter(notebook_id)\n    packagesStr \u003d \u0027,\u0027.join(packages)\n    repositoriesStr \u003d \u0027,\u0027.join(repositories)\n    exclusionsStr \u003d \u0027,\u0027.join(exclusions)\n    \n    if len(repositories) \u003e 0:\n        fout.write(\u0027\\n\\n%s *** Attached Additional Repos - %s ***\u0027%(comment,repositoriesStr))\n    \n    #print(\"Repositories - %s\" %(\u0027, \u0027.join(repositories)) )\n    #print(\"Packages - %s\" %(\u0027, \u0027.join(packages)) )\n    #print(\"Exclusions - %s\" %(\u0027, \u0027.join(exclusions)) )\n    #print(\"Spark Properties - \")\n    #print(sparkProperties)\n    \n    sparkConfig \u003d \"\"        \n    appName \u003d \"\"\n    notebookName \u003d notebookName.replace(\" \",\"\")\n    if moveSparkConfToSparkSession \u003d\u003d \u0027Y\u0027:\n        if outputLang \u003d\u003d \u0027PYTHON\u0027:\n            for key in sparkProperties.keys() :\n                sparkConfig \u003d sparkConfig + \"\"\"\\n\\t.config(\"%s\", \"%s\") \\\\\"\"\"%(key,sparkProperties[key]) \n            #if len(packages) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\"\\n\\t.config(\"spark.jars.packages\",\"%s\") \\\\\"\"\"%(packagesStr)\n            #if len(exclusions) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\"\\n\\t.config(\"spark.jars.excludes\",\"%s\") \\\\\"\"\"%(exclusionsStr)\n            appName \u003d \"\"\"\\n\\t.appName(\"%s\") \\\\\"\"\"%(notebookName)    \n        if outputLang \u003d\u003d \u0027R\u0027:\n            cnt \u003d 0\n            sparkConfig \u003d \", sparkConfig \u003d  list(\"\n            for key in sparkProperties.keys() :\n                cnt \u003d cnt + 1\n                if cnt \u003d\u003d 1:\n                    sparkConfig \u003d sparkConfig + \"\"\" %s\u003d\"%s\" \"\"\" %(key,sparkProperties[key]) \n                else:\n                    sparkConfig \u003d sparkConfig  + \"\"\" ,%s\u003d\"%s\" \"\"\" %(key,sparkProperties[key])\n            #if len(packages) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\" ,spark.jars.packages\u003d\"%s\" \"\"\" %(packagesStr)\n            #if len(exclusions) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\" ,spark.jars.excludes\u003d\"%s\" \"\"\" %(exclusionsStr)        \n            sparkConfig \u003d sparkConfig + \")\"    \n            #appName \u003d \"\"\" appName \u003d \"%s\" ,\"\"\"%(notebookName)   \n        if outputLang \u003d\u003d \u0027SCALA\u0027:\n            for key in sparkProperties.keys() :\n                sparkConfig \u003d sparkConfig + \"\"\"\\n%s\\t.config(\"%s\", \"%s\")\"\"\"%(comment,key,sparkProperties[key]) \n            #if len(packages) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\"\\n%s\\t.config(\"spark.jars.packages\",\"%s\")\"\"\"%(comment,packagesStr)\n            #if len(exclusions) \u003e 0:\n            #    sparkConfig \u003d sparkConfig + \"\"\"\\n%s\\t.config(\"spark.jars.excludes\",\"%s\")\"\"\"%(comment,exclusionsStr)                \n            appName \u003d \"\"\"\\n%s\\t.appName(\"%s\") \"\"\"%(comment,notebookName) \n    \n    if outputLang \u003d\u003d \u0027ALL\u0027:\n        fout.write(\u0027\\n\\n%s *** Interpreter Configs ***\u0027%(comment))\n        for key in sparkPropertiesAll.keys() :\n            fout.write(\u0027\\n%s\\t %s -\u003e %s \u0027%(comment,key,sparkPropertiesAll[key]))\n        if len(packages) \u003e 0:\n            fout.write(\u0027\\n\\n%s Interpreter Packages - %s\u0027%(comment,packagesStr))\n        if len(exclusions) \u003e 0:\n            fout.write(\u0027\\n\\n%s Interpreter Exclusions - %s\u0027%(comment,packagesStr))    \n   \n   \n        \n    if addSparkSession \u003d\u003d \u0027Y\u0027 and outputLang !\u003d \u0027ALL\u0027:\n        fout.write(\u0027\\n\\n%s *** Spark Session ***\u0027%(comment))\n        msg \u003d \"Zeppelin Backward Compatibility\"\n        text \u003d \"\"\n        if outputLang \u003d\u003d \u0027PYTHON\u0027:\n            text \u003d \"\"\"\\n\\nfrom pyspark.sql import SparkSession,SQLContext\\nspark \u003d SparkSession \\\\\\n\\t.builder \\\\%s%s\\n\\t.enableHiveSupport() \\\\\\n\\t.getOrCreate()\\n\\nsc \u003d spark.sparkContext\\n%s%s\\nsqlContext \u003d SQLContext(sc)\"\"\"%(appName,sparkConfig,comment,msg)        \n        if outputLang \u003d\u003d \u0027R\u0027:\n            appName \u003d \"\"\" appName \u003d \"%s\" ,\"\"\"%(notebookName)\n            text \u003d \"\"\"\\n\\nlibrary(SparkR)\\nsparkR.session(%s enableHiveSupport \u003d TRUE %s)\\n%s%s\\nsqlContext \u003c- sparkRSQL.init()\"\"\"%(appName,sparkConfig,comment,msg) \n        if outputLang \u003d\u003d \u0027SCALA\u0027:\n            dataSub \u003d {\u0027comment\u0027: comment, \u0027appName\u0027: appName , \u0027sparkConfig\u0027 : sparkConfig }\n            text \u003d \"\"\"\\n\\n%simport org.apache.spark.sql.{SparkSession,SQLContext}\"\"\"%(comment)\n            text \u003d text + \"\"\"\\n%(comment)sval spark \u003d SparkSession\\n\\t%(comment)s.builder()%(appName)s%(sparkConfig)s\\n\\t%(comment)s.enableHiveSupport()\\n\\t%(comment)s.getOrCreate()\"\"\"%(dataSub) \n            text \u003d text +  \"\"\"\\n\\nimport spark.implicits._\"\"\"\n            text \u003d text +  \"\"\"\\n%sval sc \u003d spark.sparkContext\\n\"\"\"%(comment)\n            text \u003d text +  \"\"\"\\n\\n%s%s\"\"\"%(comment,msg)\n            text \u003d text + \"\"\"\\nval sqlContext \u003d Class.forName(\"org.apache.spark.sql.SQLContext\").getConstructor(classOf[SparkContext]).newInstance(sc).asInstanceOf[SQLContext]\"\"\"\n        fout.write(text)\n\n\n\n    if generateSparkSubmit \u003d\u003d \u0027Y\u0027:\n        if outputLang \u003d\u003d \u0027SCALA\u0027:\n            cmdLine \u003d \u0027*** Submit Jobs through Analyze -\u003e Spark Command -\u003e Scala -\u003e Query Path ***\\n\u0027\n            cmdLine \u003d cmdLine + \u0027** Spark Submit Command Line Options - **\\n\u0027\n        else:    \n            cmdLine \u003d \u0027/usr/lib/spark/bin/spark-submit --master yarn-client \u0027\n        if moveSparkConfToSparkSession \u003d\u003d \u0027N\u0027:\n            if outputLang !\u003d \u0027R\u0027:\n                cmdLine \u003d cmdLine + \u0027 --name \"%s\" \u0027 %(notebookName)\n            for key in sparkProperties.keys() :\n                cmdLine \u003d cmdLine + \"\"\" --conf %s\u003d%s \"\"\"%(key,sparkProperties[key]) \n        if len(packages) \u003e 0:\n            cmdLine \u003d cmdLine + \u0027 --packages %s\u0027 %(packagesStr)\n        if len(exclusions) \u003e 0:\n            cmdLine \u003d cmdLine + \u0027 --exclude-packages %s\u0027 %(exclusionsStr)\n        if len(repositories) \u003e 0:\n            cmdLine \u003d cmdLine + \u0027 --repositories %s\u0027 %(repositoriesStr)\n        if cloudStorageFile !\u003d None:\n            if outputLang \u003d\u003d \u0027SCALA\u0027:\n                cmdLine \u003d cmdLine + \u0027\\n** Query Path - **\\n\u0027\n                cmdLine \u003d cmdLine + \u0027%s \u0027 %(cloudStorageFile)\n            else:    \n                cmdLine \u003d cmdLine + \u0027 %s \u0027 %(cloudStorageFile)\n        else:\n            cmdLine \u003d cmdLine + \u0027 \u003cpath_to_file\u003e \u0027 \n        #print(cmdLine)       \n\n    #get zeppelin default interpreter\n    #getZeppelinDefaultInterpreter(notebook_id)\n    skip_intps\u003d[\u0027sh\u0027,\u0027md\u0027,\u0027sql\u0027,\u0027dep\u0027,\u0027angular\u0027,\u0027spark.sql\u0027,\u0027knitr\u0027,\u0027spark.knitr\u0027]\n    valid_intps \u003d [\u0027pyspark\u0027,\u0027spark\u0027,\u0027r\u0027,\u0027spark.r\u0027]\n    print(\"Starting code export ...\")\n    for i in range (0, len (data[\u0027paragraphs\u0027])):\n        if \u0027title\u0027 in data[\u0027paragraphs\u0027][i] :\n            title \u003d data[\u0027paragraphs\u0027][i][\u0027title\u0027]\n        else:\n            title \u003d \u0027\u0027\n        if \u0027text\u0027 in data[\u0027paragraphs\u0027][i] :\n            para_no \u003d i + 1\n            fout.write(\u0027\\n\\n%s *** Paragraph Cell - %s ***\u0027%(comment,para_no))\n            if title !\u003d \u0027\u0027:\n                fout.write(\u0027\\n\\n%s *** Paragraph Title - %s ***\u0027%(comment,title))\n            if sys.version_info[0] \u003e 2:\n                content \u003d data[\u0027paragraphs\u0027][i][\u0027text\u0027]\n            else:\n                content \u003d data[\u0027paragraphs\u0027][i][\u0027text\u0027].encode(\u0027utf8\u0027)\n            inptFnd \u003d \u0027N\u0027\n            matchIntp \u003d \u0027\u0027\n            for intp in skip_intps:\n                if content.startswith(\u0027%\u0027 + intp):\n                    inptFnd \u003d \u0027Y\u0027\n                    matchIntp \u003d intp\n            if not data[\u0027paragraphs\u0027][i][\u0027config\u0027][\u0027enabled\u0027]:\n                fout.write(\u0027\\n%s ***Skipping paragraph code export as its disabled ***\u0027%(comment))\n            elif  inptFnd \u003d\u003d \u0027Y\u0027:   \n                fout.write(\u0027\\n%s ***Skipping as interpreter is %s ***\u0027%(comment,matchIntp))\n            else:\n                for intp in valid_intps:\n                    if content.startswith(\u0027%\u0027 + intp):   \n                       matchIntp \u003d intp\n                       inptFnd \u003d\u0027Y\u0027\n                if inptFnd \u003d\u003d \u0027Y\u0027: \n                    fout.write(\u0027\\n\u0027+content.lstrip(\u0027%\u0027 + matchIntp).lstrip())\n                else:\n                    fout.write(\u0027\\n\u0027+content)\n            \n    fout.close()\n    print(\"Done exporting code!\")\n    print(\"Writing to cloud storage ...\")\n    if cloudStorageFile !\u003d None:\n        writeToCloudStorageFile(tmpfilename,cloudStorageFile)\n    return tmpfilename,cmdLine\n\n\n           ",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:36 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1538106044240_1205989546",
      "id": "20180928-034044_229347616_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Sep 28, 2018 3:40:44 AM",
      "dateSubmitted": "May 20, 2019 3:53:14 PM",
      "dateStarted": "May 20, 2019 3:53:14 PM",
      "dateFinished": "May 20, 2019 3:53:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "3. Export the Code and write file to cloud storage",
      "text": "%pyspark\n\nfileprint,cmdLine\u003dgenerateNotebookCode(notebookName,cloudStorageFile,outputLang,addSparkSession,generateSparkSubmit,moveSparkConfToSparkSession)\n",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "lineNumbers": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v1",
      "jobName": "paragraph_1538106763886_102924841",
      "id": "20180928-035243_1468288517_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Found notebook\nTemp filename - /tmp/notebook_2019052015531558367594\nAttached Interpreters found\nFound Spark Interpreter\nStarting code export ...\nDone exporting code!\nWriting to cloud storage ...\nSuccessfully exported code to cloud storage file!\n"
      },
      "dateCreated": "Sep 28, 2018 3:52:43 AM",
      "dateSubmitted": "May 20, 2019 3:53:14 PM",
      "dateStarted": "May 20, 2019 3:53:14 PM",
      "dateFinished": "May 20, 2019 3:53:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "4. Spark Submit Command Line - Output",
      "text": "%pyspark\nprint(cmdLine)",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1538940398429_-1312897848",
      "id": "20181007-192638_196498047_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/lib/spark/bin/spark-submit --master yarn-client  --name \"SparkGraphFrames-Python\"  --conf spark.yarn.queue\u003dsbang@qubole.com  --conf spark.driver.memory\u003d2g  --conf spark.cassandra.connection.host\u003d10.0.0.5,10.0.0.38  --packages com.datastax.spark:spark-cassandra-connector_2.11:2.4.1,ml.combust.mleap:mleap-runtime_2.11:0.13.0,ml.combust.mleap:mleap-spark_2.11:0.13.0,ml.combust.mleap:mleap-spark-extension_2.11:0.13.0 --exclude-packages io.netty:netty-all --repositories http://dl.bintray.com/spark-packages/maven/,http://dl.bintray.com/spark-packages/maven s3://qubole-sbang/mydata/sparkgraphframes.py \n"
      },
      "dateCreated": "Oct 7, 2018 7:26:38 PM",
      "dateSubmitted": "May 20, 2019 3:53:14 PM",
      "dateStarted": "May 20, 2019 3:53:18 PM",
      "dateFinished": "May 20, 2019 3:53:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "title": "5. Exported Code - Output",
      "text": "%pyspark\nprintFile(fileprint)\n",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v1",
      "jobName": "paragraph_1538108601036_-690992218",
      "id": "20180928-042321_607276231_q_6R6NY5G5S81558364385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\n# *** Notebook Name - SparkGraphFrames - Python ***\n# *** Notebook Id - Z1HM2M949S1538972475 ***\n\n# *** Attached Additional Repos - http://dl.bintray.com/spark-packages/maven/,http://dl.bintray.com/spark-packages/maven ***\n\n# *** Spark Session ***\n\nfrom pyspark.sql import SparkSession,SQLContext\nspark \u003d SparkSession \\\n\t.builder \\\n\t.enableHiveSupport() \\\n\t.getOrCreate()\n\nsc \u003d spark.sparkContext\n#Zeppelin Backward Compatibility\nsqlContext \u003d SQLContext(sc)\n\n# *** Paragraph Cell - 1 ***\n\n# *** Paragraph Title - Python GraphFrames ***\n#sc.addPyFile(\u0027/usr/lib/spark/lib/graphframes.zip\u0027)\n\n\nfrom graphframes import *\n\n\nv \u003d sqlContext.createDataFrame([\n   (\"a\", \"Alice\", 34),\n   (\"b\", \"Bob\", 36),\n   (\"c\", \"Charlie\", 30),\n ], [\"id\", \"name\", \"age\"])\n # Create an Edge DataFrame with \"src\" and \"dst\" columns\ne \u003d sqlContext.createDataFrame([\n   (\"a\", \"b\", \"friend\"),\n   (\"b\", \"c\", \"follow\"),\n   (\"c\", \"b\", \"follow\"),\n ], [\"src\", \"dst\", \"relationship\"])\n# Create a GraphFrame\n\ng \u003d GraphFrame(v, e)\ng.inDegrees.show()\n\n# *** Paragraph Cell - 2 ***\ng.inDegrees.show()\ng.inDegrees.show()\n\n\n# *** Paragraph Cell - 3 ***\n# ***Skipping paragraph code export as its disabled ***\n"
      },
      "dateCreated": "Sep 28, 2018 4:23:21 AM",
      "dateSubmitted": "May 20, 2019 3:53:18 PM",
      "dateStarted": "May 20, 2019 3:53:18 PM",
      "dateFinished": "May 20, 2019 3:53:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 1000
    },
    {
      "text": "",
      "user": "sbang@qubole.com",
      "dateUpdated": "May 20, 2019 3:53:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1538983101794_790370411",
      "id": "20181008-071821_1542852169_q_6R6NY5G5S81558364385",
      "dateCreated": "Oct 8, 2018 7:18:21 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 1000
    }
  ],
  "name": "Export Notebook Code",
  "id": "6R6NY5G5S81558364385",
  "angularObjects": {
    "2E59T97NP375841550464010568:6R6NY5G5S81558364385": [],
    "2CSSZ3RZ1375841505910372553:shared_process": [],
    "2CW57XP1D375841505910372535:shared_process": [],
    "2CTGT79TT375841505910372506:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "spark"
  },
  "info": {},
  "source": "FCN"
}